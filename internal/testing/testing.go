// Copyright 2009 The Go Authors. All rights reserved.  Use of this source code
// is governed by a BSD-style license that can be found at
// https://go.googlesource.com/go/+/refs/heads/master/LICENSE.

// Based on
// https://go.googlesource.com/go/+/refs/heads/master/src/testing/testing.go

// TODO: support gosim test [-v]
// TODO: decide what to do with non-testing.T logs in gosim CLI

// TODO: support gosim test -list
// TODO: support gosim test success/fail counts?

// TODO: somehow translate testing package instead of this copy-paste mess?

package testing

import (
	"context"
	"errors"
	"fmt"
	"io"
	"log/slog"
	"regexp"
	"runtime"
	"sync"
	"sync/atomic"
	"time"

	"github.com/kmrgirish/gosim/gosimruntime"
	"github.com/kmrgirish/gosim/internal/race"
)

type writer struct{}

func (w writer) Write(p []byte) (n int, err error) {
	gosimruntime.WriteLog(p)
	return len(p), nil
}

var running sync.Map

// highPrecisionTime represents a single point in time.
// On all systems except Windows, using time.Time is fine.
type highPrecisionTime struct {
	now time.Time
}

// highPrecisionTimeNow returns high precision time for benchmarking.
func highPrecisionTimeNow() highPrecisionTime {
	return highPrecisionTime{now: time.Now()}
}

// highPrecisionTimeSince returns duration since b.
func highPrecisionTimeSince(b highPrecisionTime) time.Duration {
	return time.Since(b.now)
}

// fmtDuration returns a string representing d in the form "87.00s".
func fmtDuration(d time.Duration) string {
	return fmt.Sprintf("%.2fs simulated", d.Seconds())
}

type TB interface {
	Cleanup(func())
	Error(args ...any)
	Errorf(format string, args ...any)
	Fail()
	FailNow()
	Failed() bool
	Fatal(args ...any)
	Fatalf(format string, args ...any)
	Helper()
	Log(args ...any)
	Logf(format string, args ...any)
	Name() string
	Setenv(key, value string)
	Skip(args ...any)
	SkipNow()
	Skipf(format string, args ...any)
	Skipped() bool
	TempDir() string

	private()
}

var (
	_ TB = (*T)(nil)
	_ TB = (*B)(nil)
)

const marker = byte(0x16) // ^V for framing

type chattyPrinter struct {
	w          io.Writer
	lastNameMu sync.Mutex // guards lastName
	lastName   string     // last printed test name in chatty mode
	json       bool       // -v=json output mode
}

func newChattyPrinter(w io.Writer) *chattyPrinter {
	return &chattyPrinter{w: w, json: false /* chatty.json */}
}

// prefix is like chatty.prefix but using p.json instead of chatty.json.
// Using p.json allows tests to check the json behavior without modifying
// the global variable. For convenience, we allow p == nil and treat
// that as not in json mode (because it's not chatty at all).
func (p *chattyPrinter) prefix() string {
	if p != nil && p.json {
		return string(marker)
	}
	return ""
}

// Updatef prints a message about the status of the named test to w.
//
// The formatted message must include the test name itself.
func (p *chattyPrinter) Updatef(testName, format string, args ...any) {
	p.lastNameMu.Lock()
	defer p.lastNameMu.Unlock()

	// Since the message already implies an association with a specific new test,
	// we don't need to check what the old test name was or log an extra NAME line
	// for it. (We're updating it anyway, and the current message already includes
	// the test name.)
	p.lastName = testName
	fmt.Fprintf(p.w, p.prefix()+format, args...)
}

// Printf prints a message, generated by the named test, that does not
// necessarily mention that tests's name itself.
func (p *chattyPrinter) Printf(testName, format string, args ...any) {
	p.lastNameMu.Lock()
	defer p.lastNameMu.Unlock()

	if p.lastName == "" {
		p.lastName = testName
	} else if p.lastName != testName {
		fmt.Fprintf(p.w, "%s=== NAME  %s\n", p.prefix(), testName)
		p.lastName = testName
	}

	fmt.Fprintf(p.w, format, args...)
}

type common struct {
	mu          sync.Mutex
	helperPCs   map[uintptr]struct{}
	helperNames map[string]struct{} // helperPCs converted to function names

	barrier chan bool
	signal  chan bool

	parent *common

	chatty *chattyPrinter // A copy of chattyPrinter, if the chatty flag is set.

	cleanupName string
	cleanupPc   []uintptr

	ran      bool
	failed   bool
	skipped  bool
	done     bool
	finished bool

	level   int
	creator []uintptr

	isParallel bool

	cleanups []func()

	cleanupStarted atomic.Bool // Registered cleanup callbacks have started to execute

	hasSub atomic.Bool // whether there are sub-benchmarks.
	sub    []*T        // Queue of subtests to be run in parallel.

	lastRaceErrors  atomic.Int64 // Max value of race.Errors seen during the test or its subtests.
	raceErrorLogged atomic.Bool

	name string

	runner string

	duration time.Duration
	start    highPrecisionTime // Time test or benchmark started
}

// resetRaces updates c.parent's count of data race errors (or the global count,
// if c has no parent), and updates c.lastRaceErrors to match.
//
// Any races that occurred prior to this call to resetRaces will
// not be attributed to c.
func (c *common) resetRaces() {
	if c.parent == nil {
		c.lastRaceErrors.Store(int64(race.Errors()))
	} else {
		c.lastRaceErrors.Store(c.parent.checkRaces())
	}
}

// checkRaces checks whether the global count of data race errors has increased
// since c's count was last reset.
//
// If so, it marks c as having failed due to those races (logging an error for
// the first such race), and updates the race counts for the parents of c so
// that if they are currently suspended (such as in a call to T.Run) they will
// not log separate errors for the race(s).
//
// Note that multiple tests may be marked as failed due to the same race if they
// are executing in parallel.
func (c *common) checkRaces() (raceErrors int64) {
	raceErrors = int64(race.Errors())
	for {
		last := c.lastRaceErrors.Load()
		if raceErrors <= last {
			// All races have already been reported.
			return raceErrors
		}
		if c.lastRaceErrors.CompareAndSwap(last, raceErrors) {
			break
		}
	}

	if c.raceErrorLogged.CompareAndSwap(false, true) {
		// This is the first race we've encountered for this test.
		// Mark the test as failed, and log the reason why only once.
		// (Note that the race detector itself will still write a goroutine
		// dump for any further races it detects.)
		c.Errorf("race detected during execution of test")
	}

	// Update the parent(s) of this test so that they don't re-report the race.
	parent := c.parent
	for parent != nil {
		for {
			last := parent.lastRaceErrors.Load()
			if raceErrors <= last {
				// This race was already reported by another (likely parallel) subtest.
				return raceErrors
			}
			if parent.lastRaceErrors.CompareAndSwap(last, raceErrors) {
				break
			}
		}
		parent = parent.parent
	}

	return raceErrors
}

type T struct {
	common
	context *testContext
}

func (t *T) report() {
	if t.parent == nil {
		return
	}

	dstr := fmtDuration(t.duration)
	format := "--- %s: %s (%s)\n"
	if t.Failed() {
		t.flushToParent(t.name, format, "FAIL", t.name, dstr)
	} else if t.chatty != nil {
		if t.Skipped() {
			t.flushToParent(t.name, format, "SKIP", t.name, dstr)
		} else {
			t.flushToParent(t.name, format, "PASS", t.name, dstr)
		}
	}
}

//go:norace
func (c *common) Fail() {
	if c.parent != nil {
		c.parent.Fail()
	}
	c.mu.Lock()
	defer c.mu.Unlock()
	if c.done {
		panic("Fail in goroutine after " + c.name + " has completed")
	}
	c.failed = true
}

func (c *common) setRan() {
	if c.parent != nil {
		c.parent.setRan()
	}
	c.mu.Lock()
	defer c.mu.Unlock()
	c.ran = true
}

//go:norace
func (c *common) Failed() bool {
	c.mu.Lock()
	defer c.mu.Unlock()

	// TODO: race error count?

	return c.failed
}

func (c *common) FailNow() {
	c.Fail()

	c.mu.Lock()
	c.finished = true
	c.mu.Unlock()

	runtime.Goexit()

	// XXX: this is interesting... do we abort here or all the way up? do we make t.Run() mandatory
	// gosimruntime.SetAbortError(gosimruntime.ErrAborted)
}

func (c *common) log(level slog.Level, msg string, attrs ...slog.Attr) {
	ctx := context.Background()
	l := slog.Default()

	if !l.Enabled(ctx, level) {
		return
	}

	frame := c.frameSkip(2) // skip this function, this function's caller
	pc := frame.PC          // XXX: this is lossy if a helper got inlined. help can't do anything about that.

	// var pc uintptr
	// if !internal.IgnorePC {
	// var pcs [1]uintptr
	// skip [runtime.Callers, this function, this function's caller]
	// runtime.Callers(3, pcs[:])
	// pc = pcs[0]
	// }
	r := slog.NewRecord(time.Now(), level, msg, pc)
	r.AddAttrs(attrs...)
	l.Handler().Handle(ctx, r)
}

func (c *common) Log(a ...any) {
	// XXX: important, use fmt.Sprintln, that will be translated, to handle eg. reflect
	c.log(slog.LevelInfo, fmt.Sprint(a...), slog.String("method", "t.Log"))
}

func (c *common) Logf(format string, a ...any) {
	c.log(slog.LevelInfo, fmt.Sprintf(format, a...), slog.String("method", "t.Logf"))
}

func (c *common) Error(a ...any) {
	c.log(slog.LevelError, fmt.Sprint(a...), slog.String("method", "t.Error"))
	c.Fail()
}

func (c *common) Errorf(format string, a ...any) {
	c.log(slog.LevelError, fmt.Sprintf(format, a...), slog.String("method", "t.Errorf"))
	c.Fail()
}

func (c *common) Fatal(a ...any) {
	c.log(slog.LevelError, fmt.Sprint(a...), slog.String("method", "t.Fatal"))
	c.FailNow()
}

func (c *common) Fatalf(format string, a ...any) {
	c.log(slog.LevelError, fmt.Sprintf(format, a...), slog.String("method", "t.Fatalf"))
	c.FailNow()
}

func (c *common) Skip(a ...any) {
	c.log(slog.LevelInfo, fmt.Sprint(a...), slog.String("method", "t.Skip"))
	c.SkipNow()
}

func (c *common) Skipf(format string, a ...any) {
	c.log(slog.LevelInfo, fmt.Sprintf(format, a...), slog.String("method", "t.Skipf"))
	c.SkipNow()
}

func (c *common) Skipped() bool {
	c.mu.Lock()
	defer c.mu.Unlock()
	return c.skipped
}

func (c *common) SkipNow() {
	c.mu.Lock()
	c.skipped = true
	c.finished = true
	c.mu.Unlock()
	runtime.Goexit()
}

func (c *common) Name() string {
	return c.name
}

// XXX: copied from standard library... think about correctness?

// The maximum number of stack frames to go through when skipping helper functions for
// the purpose of decorating log messages.
const maxStackLen = 50

// callerName gives the function name (qualified with a package path)
// for the caller after skip frames (where 0 means the current function).
func callerName(skip int) string {
	var pc [1]uintptr
	n := runtime.Callers(skip+2, pc[:]) // skip + runtime.Callers + callerName
	if n == 0 {
		panic("testing: zero callers found")
	}
	return pcToName(pc[0])
}

func pcToName(pc uintptr) string {
	pcs := []uintptr{pc}
	frames := runtime.CallersFrames(pcs)
	frame, _ := frames.Next()
	return frame.Function
}

// frameSkip searches, starting after skip frames, for the first caller frame
// in a function not marked as a helper and returns that frame.
// The search stops if it finds a tRunner function that
// was the entry point into the test and the test is not a subtest.
// This function must be called with c.mu held.
func (c *common) frameSkip(skip int) runtime.Frame {
	// If the search continues into the parent test, we'll have to hold
	// its mu temporarily. If we then return, we need to unlock it.
	shouldUnlock := false
	defer func() {
		if shouldUnlock {
			c.mu.Unlock()
		}
	}()
	var pc [maxStackLen]uintptr
	// Skip two extra frames to account for this function
	// and runtime.Callers itself.
	n := runtime.Callers(skip+2, pc[:])
	if n == 0 {
		panic("testing: zero callers found")
	}
	frames := runtime.CallersFrames(pc[:n])
	var firstFrame, prevFrame, frame runtime.Frame
	for more := true; more; prevFrame = frame {
		frame, more = frames.Next()
		if frame.Function == "runtime.gopanic" {
			continue
		}
		if frame.Function == c.cleanupName {
			frames = runtime.CallersFrames(c.cleanupPc)
			continue
		}
		if firstFrame.PC == 0 {
			firstFrame = frame
		}
		if frame.Function == c.runner {
			// We've gone up all the way to the tRunner calling
			// the test function (so the user must have
			// called tb.Helper from inside that test function).
			// If this is a top-level test, only skip up to the test function itself.
			// If we're in a subtest, continue searching in the parent test,
			// starting from the point of the call to Run which created this subtest.
			if c.level > 1 {
				frames = runtime.CallersFrames(c.creator)
				parent := c.parent
				// We're no longer looking at the current c after this point,
				// so we should unlock its mu, unless it's the original receiver,
				// in which case our caller doesn't expect us to do that.
				if shouldUnlock {
					c.mu.Unlock()
				}
				c = parent
				// Remember to unlock c.mu when we no longer need it, either
				// because we went up another nesting level, or because we
				// returned.
				shouldUnlock = true
				c.mu.Lock()
				continue
			}
			return prevFrame
		}
		// If more helper PCs have been added since we last did the conversion
		c.mu.Lock()
		if c.helperNames == nil {
			c.helperNames = make(map[string]struct{})
			for pc := range c.helperPCs {
				c.helperNames[pcToName(pc)] = struct{}{}
			}
		}
		if _, ok := c.helperNames[frame.Function]; !ok {
			c.mu.Unlock()
			// Found a frame that wasn't inside a helper function.
			return frame
		}
		c.mu.Unlock()
	}
	return firstFrame
}

// flushToParent writes c.output to the parent after first writing the header
// with the given format and arguments.
func (c *common) flushToParent(testName, format string, args ...any) {
	p := c.parent
	p.mu.Lock()
	defer p.mu.Unlock()

	c.mu.Lock()
	defer c.mu.Unlock()

	/*
		if len(c.output) > 0 {
			// Add the current c.output to the print,
			// and then arrange for the print to replace c.output.
			// (This displays the logged output after the --- FAIL line.)
			format += "%s"
			args = append(args[:len(args):len(args)], c.output)
			c.output = c.output[:0]
		}
	*/

	if c.chatty != nil /* && (p.w == c.chatty.w || c.chatty.json) */ {
		// We're flushing to the actual output, so track that this output is
		// associated with a specific test (and, specifically, that the next output
		// is *not* associated with that test).
		//
		// Moreover, if c.output is non-empty it is important that this write be
		// atomic with respect to the output of other tests, so that we don't end up
		// with confusing '=== NAME' lines in the middle of our '--- PASS' block.
		// Neither humans nor cmd/test2json can parse those easily.
		// (See https://go.dev/issue/40771.)
		//
		// If test2json is used, we never flush to parent tests,
		// so that the json stream shows subtests as they finish.
		// (See https://go.dev/issue/29811.)
		c.chatty.Updatef(testName, format, args...)
	} else {
		// We're flushing to the output buffer of the parent test, which will
		// itself follow a test-name header when it is finally flushed to stdout.
		// fmt.Fprintf(p.w, c.chatty.prefix()+format, args...)
	}
}

func (c *common) Helper() {
	c.mu.Lock()
	defer c.mu.Unlock()
	if c.helperPCs == nil {
		c.helperPCs = make(map[uintptr]struct{})
	}
	// repeating code from callerName here to save walking a stack frame
	var pc [1]uintptr
	n := runtime.Callers(2, pc[:]) // skip runtime.Callers + Helper
	if n == 0 {
		panic("testing: zero callers found")
	}
	if _, found := c.helperPCs[pc[0]]; !found {
		c.helperPCs[pc[0]] = struct{}{}
		c.helperNames = nil // map will be recreated next time it is needed
	}
}

func (c *common) Cleanup(f func()) {
	var pc [maxStackLen]uintptr
	// Skip two extra frames to account for this function and runtime.Callers itself.
	n := runtime.Callers(2, pc[:])
	cleanupPc := pc[:n]

	fn := func() {
		defer func() {
			c.mu.Lock()
			defer c.mu.Unlock()
			c.cleanupName = ""
			c.cleanupPc = nil
		}()

		name := callerName(0)
		c.mu.Lock()
		c.cleanupName = name
		c.cleanupPc = cleanupPc
		c.mu.Unlock()

		f()
	}

	c.mu.Lock()
	defer c.mu.Unlock()
	c.cleanups = append(c.cleanups, fn)
}

// panicHandling controls the panic handling used by runCleanup.
type panicHandling int

const (
	normalPanic panicHandling = iota
	recoverAndReturnPanic
)

// runCleanup is called at the end of the test.
// If ph is recoverAndReturnPanic, it will catch panics, and return the
// recovered value if any.
func (c *common) runCleanup(ph panicHandling) (panicVal any) {
	c.cleanupStarted.Store(true)
	defer c.cleanupStarted.Store(false)

	if ph == recoverAndReturnPanic {
		defer func() {
			panicVal = recover()
		}()
	}

	// Make sure that if a cleanup function panics,
	// we still run the remaining cleanup functions.
	defer func() {
		c.mu.Lock()
		recur := len(c.cleanups) > 0
		c.mu.Unlock()
		if recur {
			c.runCleanup(normalPanic)
		}
	}()

	for {
		var cleanup func()
		c.mu.Lock()
		if len(c.cleanups) > 0 {
			last := len(c.cleanups) - 1
			cleanup = c.cleanups[last]
			c.cleanups = c.cleanups[:last]
		}
		c.mu.Unlock()
		if cleanup == nil {
			return nil
		}
		cleanup()
	}
}

func (c *common) TempDir() string {
	panic("not implemented")
}

func (c *common) private() {}

func (t *T) Parallel() {
	if t.isParallel {
		panic("testing: t.Parallel called multiple times")
	}
	t.isParallel = true
	if t.parent.barrier == nil {
		// T.Parallel has no effect when fuzzing.
		// Multiple processes may run in parallel, but only one input can run at a
		// time per process so we can attribute crashes to specific inputs.
		return
	}

	// We don't want to include the time we spend waiting for serial tests
	// in the test duration. Record the elapsed time thus far and reset the
	// timer afterwards.
	t.duration += highPrecisionTimeSince(t.start)

	// Add to the list of tests to be released by the parent.
	t.parent.sub = append(t.parent.sub, t)

	// Report any races during execution of this test up to this point.
	//
	// We will assume that any races that occur between here and the point where
	// we unblock are not caused by this subtest. That assumption usually holds,
	// although it can be wrong if the test spawns a goroutine that races in the
	// background while the rest of the test is blocked on the call to Parallel.
	// If that happens, we will misattribute the background race to some other
	// test, or to no test at all — but that false-negative is so unlikely that it
	// is not worth adding race-report noise for the common case where the test is
	// completely suspended during the call to Parallel.
	t.checkRaces()

	if t.chatty != nil {
		t.chatty.Updatef(t.name, "=== PAUSE %s\n", t.name)
	}
	running.Delete(t.name)

	t.signal <- true   // Release calling test.
	<-t.parent.barrier // Wait for the parent test to complete.
	t.context.waitParallel()

	if t.chatty != nil {
		t.chatty.Updatef(t.name, "=== CONT  %s\n", t.name)
	}
	running.Store(t.name, highPrecisionTimeNow())
	t.start = highPrecisionTimeNow()

	// Reset the local race counter to ignore any races that happened while this
	// goroutine was blocked, such as in the parent test or in other parallel
	// subtests.
	//
	// (Note that we don't call parent.checkRaces here:
	// if other parallel subtests have already introduced races, we want to
	// let them report those races instead of attributing them to the parent.)
	t.lastRaceErrors.Store(int64(race.Errors()))
}

func (t *T) Setenv(key, value string) {
	panic("not implemented")
}

type B struct {
	common
}

func (b *B) Setenv(key, value string) {
	panic("not implemented")
}

func (t *T) Deadline() (time.Time, bool) {
	return time.Time{}, false
}

var errNilPanicOrGoexit = errors.New("test executed panic(nil) or runtime.Goexit")

func tRunner(t *T, fn func(t *T)) {
	t.runner = callerName(0)

	defer func() {
		t.checkRaces()

		// TODO(#61034): This is the wrong place for this check.
		if t.Failed() {
			// TODO: numFailed (numSkipped etc?)
			// numFailed.Add(1)
		}

		err := recover()
		signal := true

		t.mu.Lock()
		finished := t.finished
		t.mu.Unlock()

		if !finished && err == nil {
			err = errNilPanicOrGoexit
			for p := t.parent; p != nil; p = p.parent {
				p.mu.Lock()
				finished = p.finished
				p.mu.Unlock()
				if finished {
					// TODO: parallel check
					signal = false
					break
				}
			}
		}

		// TODO: err != nil && fuzzing

		didPanic := false
		defer func() {
			if didPanic {
				return
			}
			if err != nil {
				panic(err)
			}
			running.Delete(t.name)
			t.signal <- signal
		}()

		doPanic := func(err any) {
			t.Fail()
			if r := t.runCleanup(recoverAndReturnPanic); r != nil {
				t.Logf("cleanup panicked with %v", r)
			}
			for root := &t.common; root.parent != nil; root = root.parent {
				root.mu.Lock()
				root.duration += highPrecisionTimeSince(root.start)
				d := root.duration
				root.mu.Unlock()
				root.flushToParent(root.name, "--- FAIL: %s (%s)\n", root.name, fmtDuration(d))
				if r := root.parent.runCleanup(recoverAndReturnPanic); r != nil {
					// TODO: huh
					// fmt.Fprintf(root.parent.w, "cleanup panicked with %v", r)
				}
			}
			didPanic = true
			panic(err)
		}
		if err != nil {
			doPanic(err)
		}

		t.duration += highPrecisionTimeSince(t.start)

		if len(t.sub) > 0 {
			// Run parallel subtests.

			// Decrease the running count for this test and mark it as no longer running.
			t.context.release()
			running.Delete(t.name)

			// Release the parallel subtests.
			close(t.barrier)
			// Wait for subtests to complete.
			for _, sub := range t.sub {
				<-sub.signal
			}

			// Run any cleanup callbacks, marking the test as running
			// in case the cleanup hangs.
			cleanupStart := highPrecisionTimeNow()
			running.Store(t.name, cleanupStart)
			err := t.runCleanup(recoverAndReturnPanic)
			t.duration += highPrecisionTimeSince(cleanupStart)
			if err != nil {
				doPanic(err)
			}
			t.checkRaces()
			if !t.isParallel {
				// Reacquire the count for sequential tests. See comment in Run.
				t.context.waitParallel()
			}
		} else if t.isParallel {
			// Only release the count for this test if it was run as a parallel
			// test. See comment in Run method.
			t.context.release()
		}

		t.report()
		t.done = true // XXX: not locked because??
		if t.parent != nil && !t.hasSub.Load() {
			t.setRan()
		}
	}()
	defer func() {
		if len(t.sub) == 0 {
			t.runCleanup(normalPanic)
		}
	}()

	t.start = highPrecisionTimeNow()
	t.resetRaces() // XXX: TODO
	fn(t)

	// code beyond here will not be executed when FailNow is invoked
	t.mu.Lock()
	t.finished = true
	t.mu.Unlock()
}

// Run runs f as a subtest of t called name. It runs f in a separate goroutine
// and blocks until f returns or calls t.Parallel to become a parallel test.
// Run reports whether f succeeded (or at least did not fail before calling t.Parallel).
//
// Run may be called simultaneously from multiple goroutines, but all such calls
// must return before the outer test function for t returns.
func (t *T) Run(name string, f func(t *T)) bool {
	// if t.cleanupStarted.Load() {
	// panic("testing: t.Run called during t.Cleanup")
	// }

	t.hasSub.Store(true)
	testName, ok, _ := t.context.match.fullName(&t.common, name)
	if !ok /* || shouldFailFast() */ {
		return true
	}
	// Record the stack trace at the point of this call so that if the subtest
	// function - which runs in a separate stack - is marked as a helper, we can
	// continue walking the stack into the parent test.
	var pc [maxStackLen]uintptr
	n := runtime.Callers(2, pc[:])
	t = &T{
		common: common{
			barrier: make(chan bool),
			signal:  make(chan bool, 1),
			name:    testName,
			parent:  &t.common,
			level:   t.level + 1,
			creator: pc[:n],
			chatty:  t.chatty,
		},
		context: t.context,
	}
	// t.w = indenter{&t.common}

	if t.chatty != nil {
		t.chatty.Updatef(t.name, "=== RUN   %s (seed %d)\n", t.name, gosimruntime.Seed())
	}
	running.Store(t.name, highPrecisionTimeNow())

	// Instead of reducing the running count of this test before calling the
	// tRunner and increasing it afterwards, we rely on tRunner keeping the
	// count correct. This ensures that a sequence of sequential tests runs
	// without being preempted, even when their parent is a parallel test. This
	// may especially reduce surprises if *parallel == 1.
	go tRunner(t, f)

	// The parent goroutine will block until the subtest either finishes or calls
	// Parallel, but in general we don't know whether the parent goroutine is the
	// top-level test function or some other goroutine it has spawned.
	// To avoid confusing false-negatives, we leave the parent in the running map
	// even though in the typical case it is blocked.

	if !<-t.signal {
		// At this point, it is likely that FailNow was called on one of the
		// parent tests by one of the subtests. Continue aborting up the chain.
		runtime.Goexit()
	}

	// if t.chatty != nil && t.chatty.json {
	// t.chatty.Updatef(t.parent.name, "=== NAME  %s\n", t.parent.name)
	// }
	return !t.failed
}

// testContext holds all fields that are common to all tests. This includes
// synchronization primitives to run at most *parallel tests.
type testContext struct {
	match    *matcher
	deadline time.Time

	// isFuzzing is true in the context used when generating random inputs
	// for fuzz targets. isFuzzing is false when running normal tests and
	// when running fuzz tests as unit tests (without -fuzz or when -fuzz
	// does not match).
	isFuzzing bool

	mu sync.Mutex

	// Channel used to signal tests that are ready to be run in parallel.
	startParallel chan bool

	// running is the number of tests currently running in parallel.
	// This does not include tests that are waiting for subtests to complete.
	running int

	// numWaiting is the number tests waiting to be run in parallel.
	numWaiting int

	// maxParallel is a copy of the parallel flag.
	maxParallel int
}

func newTestContext(maxParallel int, m *matcher) *testContext {
	return &testContext{
		match:         m,
		startParallel: make(chan bool),
		maxParallel:   maxParallel,
		running:       1, // Set the count to 1 for the main (sequential) test.
	}
}

func (c *testContext) waitParallel() {
	c.mu.Lock()
	if c.running < c.maxParallel {
		c.running++
		c.mu.Unlock()
		return
	}
	c.numWaiting++
	c.mu.Unlock()
	<-c.startParallel
}

func (c *testContext) release() {
	c.mu.Lock()
	if c.numWaiting == 0 {
		c.running--
		c.mu.Unlock()
		return
	}
	c.numWaiting--
	c.mu.Unlock()
	c.startParallel <- true // Pick a waiting test to be run.
}

func runTests(match string, skip string, tests []InternalTest) (ran, ok bool) {
	ok = true

	parallel := 1 // XXX: pass later?

	ctx := newTestContext(parallel, newMatcher(regexp.MatchString, match, "-test.run", skip))
	t := &T{
		common: common{
			signal:  make(chan bool, 1),
			barrier: make(chan bool),
			// w:       os.Stdout,
			chatty: newChattyPrinter(writer{}),
		},
		context: ctx,
	}
	tRunner(t, func(t *T) {
		for _, test := range tests {
			t.Run(test.Name, test.F)
		}
	})
	select {
	case <-t.signal:
	default:
		panic("internal error: tRunner exited without sending on t.signal")
	}
	ok = ok && !t.Failed()
	ran = ran || t.ran

	return ran, ok
}

func Entrypoint(match string, skip string, tests []gosimruntime.Test) bool {
	var parsedTests []InternalTest
	for _, test := range tests {
		parsedTests = append(parsedTests, InternalTest{
			Name: test.Name,
			F:    test.Test.(func(*T)),
		})
	}
	_, ok := runTests(match, skip, parsedTests)
	return ok
}

func Short() bool {
	return false
}
